{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys, traceback\n",
    "import argparse\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "import glob\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from pybedtools import BedTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO RUN\n",
    "\n",
    "# python script input_file sample_id\n",
    "\n",
    "# python /dors/capra_lab/fongsl/enh_age/bin/age_enhancers.py UBERON_0002372_tonsil_expressed_enhancers.bed UBERON0002372\n",
    "\n",
    "\n",
    "###\n",
    "#   arguments\n",
    "###\n",
    "arg_parser = argparse.ArgumentParser(description=\"Calculate enhancer age.\")\n",
    "\n",
    "arg_parser.add_argument(\"region_file_1\", help='bed file 1 (enhancers to age) w/ full path')\n",
    "\n",
    "arg_parser.add_argument(\"sample_id\", help='str label for files')\n",
    "\n",
    "arg_parser.add_argument(\"-i\", \"--iters\", type=int, default=100,\n",
    "                        help='number of simulation iterations; default=100')\n",
    "\n",
    "arg_parser.add_argument(\"-s\", \"--species\", type=str, default='hg19', choices=['hg19', 'hg38', 'mm10'],\n",
    "                        help='species and assembly; default=hg19')\n",
    "\n",
    "arg_parser.add_argument(\"-n\", \"--num_threads\", type=int,\n",
    "                        help='number of threads; default=SLURM_CPUS_PER_TASK or 1')\n",
    "\n",
    "arg_parser.add_argument(\"--print_counts_to\", type=str, default=None,\n",
    "                        help=\"print expected counts to file\")\n",
    "\n",
    "args = arg_parser.parse_args()\n",
    "\n",
    "TEST_ENH = args.region_file_1\n",
    "SAMPLE_ID = args.sample_id\n",
    "COUNT_FILENAME = args.print_counts_to\n",
    "ITERATIONS = args.iters\n",
    "SPECIES = args.species\n",
    "TEST_PATH = \"/\".join(TEST_ENH.split(\"/\")[:-1])\n",
    "\n",
    "AGE_OUTFILE =\"%s/%s_enh_ages.bed\" %(TEST_PATH, SAMPLE_ID)\n",
    "SHUFFLE_ID = \"Shuf-%s\" % SAMPLE_ID\n",
    "\n",
    "# calculate the number of threads\n",
    "\n",
    "if args.num_threads:\n",
    "    num_threads = args.num_threads\n",
    "else:\n",
    "    num_threads = int(os.getenv('SLURM_CPUS_PER_TASK', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "#   functions\n",
    "###\n",
    "\n",
    "def loadConstants(species):  # note chrom.sizes not used in current implementation | 2018.10.29\n",
    "    return {'hg19': (\"/dors/capra_lab/users/bentonml/data/dna/hg19/hg19_blacklist_gap.bed\", \"/dors/capra_lab/data/dna/human/hg19/hg19_trim.chrom.sizes\"),\n",
    "            'hg38': (\"/dors/capra_lab/users/bentonml/data/dna/hg38/hg38_blacklist_gap.bed\", \"/dors/capra_lab/data/dna/human/hg38/hg38_trim.chrom.sizes\"),\n",
    "            'mm10': (\"/dors/capra_lab/users/bentonml/data/dna/mm10/mm10_blacklist_gap.bed\", \"/dors/capra_lab/data/dna/mouse/mm10/mm10_trim.chrom.sizes\")\n",
    "            }[species]\n",
    "\n",
    "\n",
    "def enh_age(test_enh, sample_id, test_path, species):\n",
    "    os.chdir(test_path)\n",
    "    \n",
    "    cut_file = \"standard_%s.bed\" % sample_id # standard.bed\n",
    "    cut_cmd = \"cut -f 1-3 %s > ./%s\" % (test_enh, cut_file) # standardize the input file\n",
    "    \n",
    "    os.system(cut_cmd)\n",
    "    \n",
    "    chr_cmd = \"awk '{print >$1\\\"_%s_temp.bed\\\"}' %s\" % (sample_id, cut_file) # split test into chrN.bed files\n",
    "    os.system(chr_cmd)\n",
    "    \n",
    "    rm_cut_file = \"rm ./%s\" % cut_file\n",
    "    os.system(rm_cut_file)\n",
    "    \n",
    "    enh_chr_list = glob.glob(\"%s/chr*_%s_temp.bed\" % (test_path, sample_id)) # glob chromosomes\n",
    "\n",
    "    for enh_chr in enh_chr_list: # intersect test chrN.bed w/ syntenic block\n",
    "        chr_num = (enh_chr.split(\"/\")[-1]).split(\"_\")[0]\n",
    "        \n",
    "        syn_path = \"/dors/capra_lab/data/ucsc/%s/synteny_age_bkgd_%s/\" %(species, species)\n",
    "        syn_file= (\"%s%s_syn_age.bed\" % (syn_path, chr_num))\n",
    "        \n",
    "        outfile = \"%s/%s_%s_ages.bed\" %(test_path, chr_num, sample_id,)\n",
    "        bed_cmd = \"bedtools intersect -a %s -b %s -wao > %s\" % (enh_chr, syn_file, outfile)\n",
    "        \n",
    "        os.system(bed_cmd)\n",
    "\n",
    "    concat_cmd = \"cat %s/chr*_%s_ages.bed > %s/%s_enh_ages.bed\" % (test_path, sample_id, test_path, sample_id)#concatenate chromosomes\n",
    "    print(\"concatenating aged enhancer chromosome files\", concat_cmd)\n",
    "    os.system(concat_cmd)\n",
    "    \n",
    "    clean_up_temp = \"rm %s/chr*_%s_*.bed\" % (test_path, sample_id) # remove old files\n",
    "    os.system(clean_up_temp)\n",
    "\n",
    "def break_tfbs(sample_id, test_path):\n",
    "    print(sample_id, test_path)\n",
    "    outpath = \"%s/breaks/\" % test_path # mkdir ./break/\n",
    "    cmd = \"mkdir %s\" % outpath\n",
    "    os.system(cmd)\n",
    "\n",
    "    df = pd.read_csv(\"%s/%s_enh_ages.bed\"%(test_path, sample_id),\\\n",
    "                         sep = '\\t', header = -1 , low_memory=False) # open up the bed file and assemble breaks.\n",
    "\n",
    "    df.columns = [\"chr_enh\", # rename the columns\n",
    "                  \"start_enh\", \n",
    "                  \"end_enh\",\n",
    "                  \"chr_syn\",\n",
    "                  \"start_syn\",\n",
    "                  \"end_syn\",\n",
    "                  \"strand\",\n",
    "                  \"ref\",\n",
    "                  \"num_species\",\n",
    "                  \"len_syn\",\n",
    "                  \"mrca\",\n",
    "                  \"patr\",\n",
    "                  \"len_syn_overlap\"]\n",
    "    \n",
    "    df[\"test_id\"] = df[\"chr_enh\"] + \":\" + df[\"start_enh\"].map(str) + \"-\" + df[\"end_enh\"].map(str) # add a test_id\n",
    "    \n",
    "    no_syn = df.loc[df[\"len_syn_overlap\"]<1] # remove \".\" coordinates non-overlapping with syntenic blocks\n",
    "\n",
    "    if len(no_syn) >1: # save all coordinates not overlapping syntenic blocks\n",
    "        no_syn.to_csv(\"%s%s_no_syn_alignment_tiles.txt\" % (outpath, sample_id),\\\n",
    "                      sep = '\\t', header = True, index = False) # write non-overlapping regions\n",
    "    df = df.loc[df[\"len_syn_overlap\"]>0] # remove all the records that do not overlap syntenic blocks\n",
    "\n",
    "    print(\"unique enhancers =\", len(df[\"test_id\"].unique())) # count the number of enhancers\n",
    "    \n",
    "    print(\"# rows to reduce and assemble breaks = \", len(df.drop_duplicates())) # drop duplicates\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    #####\n",
    "    # prepare to join breaks\n",
    "    #####\n",
    "\n",
    "    df[\"mrca\"] = df[\"mrca\"].astype(float).round(3) # round MRCA distance\n",
    "\n",
    "    id_list = df[\"test_id\"].unique() # get enh_ids\n",
    "\n",
    "    #####\n",
    "    # start the break assembly\n",
    "    #####\n",
    "    start = datetime.now()\n",
    "    print(\"Start assembling breaks\", start)\n",
    "\n",
    "    df_dict = {} # collect all the information in one dictionary\n",
    "\n",
    "    val = 0\n",
    "    write_val = 500\n",
    "\n",
    "    for i in id_list:\n",
    "\n",
    "        test = df[[\"chr_enh\",  # subset dataset to one enhancer ID\n",
    "                   \"start_enh\", \n",
    "                   \"end_enh\",\n",
    "                   \"chr_syn\",\n",
    "                   \"start_syn\",\n",
    "                   \"end_syn\",\n",
    "                   \"mrca\",\n",
    "                   \"len_syn_overlap\",\n",
    "                   \"test_id\"]].loc[df[\"test_id\"]==i].drop_duplicates().sort_values([\"chr_syn\",\n",
    "                   \"start_syn\",\n",
    "                   \"end_syn\"]).reset_index()\n",
    "\n",
    "        new_data = test[[\"chr_enh\",  # dataframe for reassembled breaks\n",
    "                         \"start_enh\", \n",
    "                         \"end_enh\",\n",
    "                         \"test_id\",\n",
    "                         \"chr_syn\"]].drop_duplicates()\n",
    "\n",
    "        collect_df = pd.DataFrame()\n",
    "\n",
    "        age_seg = [(age, sum(1 for i in rows)) for age,rows in groupby(test[\"mrca\"])]\n",
    "\n",
    "        start = test[\"start_enh\"].astype(int)\n",
    "        end = test[\"end_enh\"].astype(int)\n",
    "\n",
    "        df_index = 0\n",
    "        seg_index = 0\n",
    "        core_age = max(i for i, k in age_seg) # get the max age\n",
    "        core_age = round(core_age, 3) # REDUNDANCY, but just in case\n",
    "\n",
    "        for tup in age_seg:\n",
    "            age, idx = tup\n",
    "            age = round(age, 3) # REDUNDANCY, but just in case\n",
    "\n",
    "\n",
    "            if len(age_seg)== 1: # simple enhancers\n",
    "\n",
    "                new_data[\"start_syn\"] = start\n",
    "                new_data[\"end_syn\"] = end\n",
    "                new_data[\"mrca_seg\"] = age\n",
    "                new_data[\"seg_index\"] = 0 # index syntenic segments\n",
    "                new_data[\"core\"] = 1 # binary core measure\n",
    "                new_data[\"core_remodeling\"] = 0 # binary core remodeling measure\n",
    "                collect_df= collect_df.append(new_data)\n",
    "\n",
    "            else: # complex enhancers\n",
    "\n",
    "                new_data[\"seg_index\"]= seg_index # index syntenic segments\n",
    "                new_data[\"core_remodeling\"] = 1 # binary core remodeling measure\n",
    "\n",
    "                if age == core_age: # mark the core\n",
    "                    new_data[\"core\"] = 1\n",
    "                    \n",
    "                else:\n",
    "                    new_data[\"core\"] = 0\n",
    "\n",
    "                if seg_index == 0: # trim first syntenic block to start\n",
    "                    new_data[\"start_syn\"] = start\n",
    "                    new_data[\"end_syn\"] = test.loc[idx-1, \"end_syn\"]\n",
    "\n",
    "                    new_data[\"mrca_seg\"] = age\n",
    "                    collect_df= collect_df.append(new_data)\n",
    "\n",
    "                elif seg_index == len(age_seg)-1: # trim last syntenic block\n",
    "                    new_data[\"mrca_seg\"] = age\n",
    "                    new_data[\"start_syn\"] = test.loc[df_index, \"start_syn\"]\n",
    "                    new_data[\"end_syn\"] = end\n",
    "                    collect_df= collect_df.append(new_data)\n",
    "\n",
    "                else: # deal with all the blocks in between first and last syntenic block\n",
    "                    new_data[\"mrca_seg\"] = age\n",
    "                    new_data[\"start_syn\"]= test.loc[df_index, \"start_syn\"]\n",
    "                    new_data[\"end_syn\"]= test.loc[df_index + idx -1, \"end_syn\"]\n",
    "                    collect_df= collect_df.append(new_data)\n",
    "\n",
    "                df_index +=idx # go to next index\n",
    "                seg_index +=1 # count age segments\n",
    "\n",
    "        df_dict[i] = collect_df\n",
    "        val +=1\n",
    "\n",
    "        if val == write_val: #periodically write the file\n",
    "            temp = pd.concat(df_dict.values(), sort = True)\n",
    "            temp = temp[['chr_syn',\n",
    "                         'start_syn',\n",
    "                         'end_syn',\n",
    "                         'test_id',\n",
    "                         'chr_enh',\n",
    "                         'start_enh',\n",
    "                         'end_enh',\n",
    "                         'seg_index',\n",
    "                         'core_remodeling',\n",
    "                         'core',\n",
    "                         'mrca_seg']]\n",
    "            temp_out = \"%s%s_age_breaks.bed\" % (outpath, sample_id)\n",
    "            with open(temp_out, 'a') as f:\n",
    "                temp.to_csv(f, sep = '\\t', header = False, index = False) # write last enhancers   \n",
    "\n",
    "            write_val += 500 # re-up write val\n",
    "            df_dict = {} # collect all the information in one dictionary\n",
    "            \n",
    "    temp = pd.concat(df_dict.values(), sort = True)\n",
    "    temp = temp[['chr_syn',\n",
    "                 'start_syn',\n",
    "                 'end_syn',\n",
    "                 'test_id',\n",
    "                 'chr_enh',\n",
    "                 'start_enh',\n",
    "                 'end_enh',\n",
    "                 'seg_index',\n",
    "                 'core_remodeling',\n",
    "                 'core',\n",
    "                 'mrca_seg']]\n",
    "    temp_out = \"%s%s_age_breaks.bed\" % (outpath, sample_id)\n",
    "    with open(temp_out, 'a') as f:\n",
    "        temp.to_csv(f, sep = '\\t', header = False, index = False) # write last enhancers   \n",
    "\n",
    "    print(\"Finished assembling breaks\", datetime.now())\n",
    "    \n",
    "def tfbs_density(sample_id, test_path):\n",
    "\n",
    "    in_path = \"%s/breaks/\" % test_path \n",
    "\n",
    "    out_path = \"%s/tfbs/\"% test_path\n",
    "\n",
    "    #####\n",
    "    # prepare workspace\n",
    "    #####\n",
    "    core_breaks_file = \"%s%s_age_breaks.bed\"% (in_path, sample_id)\n",
    "\n",
    "    temp_val = 0\n",
    "\n",
    "    tfbs_file = \"/dors/capra_lab/data/encode/midpeaks_wgEncodeRegTfbsClusteredV3.bed\" # TFBS data, 30bp trimmed ChIP-Peaks\n",
    "\n",
    "    mkdir_cmd = \"mkdir %s\" %out_path # make the out_path directory\n",
    "    os.system(mkdir_cmd)\n",
    "\n",
    "    ########\n",
    "    # PART 1 - sort bed file\n",
    "    ########\n",
    "\n",
    "    sort_cmd = \"sort -k5,5 -k2,2 -k3,3 -k6,6 -k7,7 %s >t && mv t %s\" % (core_breaks_file, core_breaks_file)\n",
    "    os.system(sort_cmd)\n",
    "\n",
    "    ########\n",
    "    # PART 2 - Bed intersect file with TFBS\n",
    "    ########\n",
    "\n",
    "    outfile_wao = \"%s%s_x_raw_tfbs_midpeak.bed\"% (out_path, sample_id)\n",
    "\n",
    "    bed_cmd = \"bedtools intersect -a %s -b %s -wao > %s\" % (core_breaks_file, tfbs_file, outfile_wao)\n",
    "\n",
    "    os.system(bed_cmd)\n",
    "\n",
    "    ########\n",
    "    # open the sample_id file\n",
    "    ########\n",
    "\n",
    "    enh = pd.read_csv(outfile_wao, header= -1, sep= '\\t',low_memory = False)\n",
    "    print(len(enh), \"raw df len\")\n",
    "    ########\n",
    "    # reformat file to get columns/data you want\n",
    "    ########\n",
    "\n",
    "    print(\"the number syntenic blocks that did not overlap TFBS\" ,\\\n",
    "          len(enh[20].loc[enh[20]==0]))\n",
    "\n",
    "    #select the columns you want for analyzing break density\n",
    "    enh = enh[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 20]].drop_duplicates()\n",
    "\n",
    "    # rename the columns\n",
    "    enh.columns = [\"chr_syn\", \n",
    "                   \"start_syn\", \n",
    "                   \"end_syn\", \n",
    "                   \"enh_id\", \n",
    "                   \"chr_enh\", \n",
    "                   \"start_enh\", \n",
    "                   \"end_enh\", \n",
    "                   \"seg_index\", \n",
    "                   \"core_remodeling\", \n",
    "                   \"core\", \n",
    "                   \"mrca\",\n",
    "                   \"chr_tf\", \n",
    "                   \"start_tf\", \n",
    "                   \"end_tf\", \n",
    "                   \"tf\",  \n",
    "                   \"len_tfbs_overlap\"]\n",
    "\n",
    "    # assign a unique syn id\n",
    "    enh[\"syn_id\"] = enh[\"chr_syn\"] + \":\" + enh[\"start_syn\"].map(str) + \"-\" + enh[\"end_syn\"].map(str)\n",
    "\n",
    "    # assign unique tf_id\n",
    "    enh[\"tf_id\"] = enh[\"chr_tf\"] + \":\" + enh[\"start_tf\"].map(str) + \"-\" + enh[\"end_tf\"].map(str) +\"_\"+ enh[\"tf\"]\n",
    "\n",
    "    print(\"the number of unique enhancers\", len(enh[\"enh_id\"].unique()))\n",
    "\n",
    "    enh_tf_list = enh[\"enh_id\"].unique() # make a list of unique enhancer ids\n",
    "\n",
    "    if temp_val > 0:\n",
    "        print(\"remove enh_ids that have already been done\", temp_val)\n",
    "        enh_tf_list = enh_tf_list[(temp_val):]\n",
    "        val = temp_val\n",
    "    else:\n",
    "        enh_tf_list = enh_tf_list\n",
    "        val = 0\n",
    "\n",
    "    # transform the dtypes for quantitative values\n",
    "    enh[[\"start_enh\", \n",
    "         \"end_enh\",\n",
    "         \"start_tf\",\n",
    "         \"start_syn\", \n",
    "         \"end_syn\", \n",
    "         \"end_tf\", \n",
    "         \"seg_index\", \n",
    "         \"core_remodeling\", \n",
    "         \"core\",\"len_tfbs_overlap\"]] = enh[[\"start_enh\", \n",
    "         \"end_enh\",\n",
    "         \"start_tf\",\n",
    "         \"start_syn\", \n",
    "         \"end_syn\", \n",
    "         \"end_tf\", \n",
    "         \"seg_index\", \n",
    "         \"core_remodeling\", \n",
    "         \"core\",\"len_tfbs_overlap\"]].astype(int)\n",
    "\n",
    "    enh[\"mrca\"] = enh[\"mrca\"].astype(float).round(3)\n",
    "    enh[\"enh_len\"]= enh[\"end_enh\"]- enh[\"start_enh\"] # enhancer lengths\n",
    "    enh[\"syn_len\"]= enh[\"end_syn\"]- enh[\"start_syn\"] # syntenic lengths\n",
    "\n",
    "    ########\n",
    "    # calculate TFBS density\n",
    "    ########\n",
    "\n",
    "    ########\n",
    "    # count tfs associated with enhancers\n",
    "    ########\n",
    "    tfenh_base = enh[[\"chr_enh\", \"start_enh\", \"end_enh\", \"enh_id\", \"enh_len\"]].drop_duplicates()\n",
    "\n",
    "    tfenh = enh.loc[enh[\"len_tfbs_overlap\"]>=6].groupby([\"enh_id\", \"enh_len\"])[\"tf\"].count().reset_index()\n",
    "\n",
    "    tfenh.columns=['enh_id','enh_len','enh_tf_count'] # rename columns\n",
    "\n",
    "    tfenh[\"enh_tf_den\"] = tfenh[\"enh_tf_count\"].divide(tfenh[\"enh_len\"]) # enhancer tfbs density\n",
    "    tfenh = tfenh[['enh_id','enh_tf_count', 'enh_tf_den']] # keep only essential info from df\n",
    "\n",
    "    # count uniq tfs associated with enhancers\n",
    "\n",
    "    tfenhunq = enh.loc[enh[\"len_tfbs_overlap\"]>=6].groupby([\"enh_id\", \"enh_len\"])[\"tf\"].unique().reset_index()\n",
    "\n",
    "    tfenhunq[\"enh_tf_unq_count\"] = tfenhunq[\"tf\"].apply(lambda x: len(x))# count the number of unique TFs\n",
    "\n",
    "    tfenhunq[\"enh_tf_unq_den\"] = tfenhunq[\"enh_tf_unq_count\"].divide(tfenhunq[\"enh_len\"])# enhancer tfbs density\n",
    "    tfenhunq = tfenhunq[['enh_id','enh_tf_unq_count', 'enh_tf_unq_den']] # keep only essential info from df\n",
    "\n",
    "    # reduce base, tfden, tfuniqden dataframes\n",
    "\n",
    "    enhdfs = [tfenh_base, tfenh, tfenhunq]\n",
    "    enh_merged = reduce(lambda left,right: pd.merge(left,right,on=[\"enh_id\"],\n",
    "                                                how='left'), enhdfs)\n",
    "\n",
    "    print(\"# total enhacers\", len(tfenh_base))\n",
    "    print(\"# enh overlapping TF >=6bp\", len(tfenh)) # exclude any enhancers w/o tfbs overlapping >=6bp\n",
    "    print(\"# unq enh overlapping TF >=6bp\", len(tfenhunq)) # exclude any enhancer w/o uniq tfbs overlapping >=6bp\n",
    "    print(\"len merged enh\", len(enh_merged)) # the left-merged df\n",
    "\n",
    "    enh_merged.to_csv(\"%s%s_enh_tfbs_density.bed\" % (out_path, sample_id), sep= '\\t', header = True, index = False)\n",
    "\n",
    "    ########\n",
    "    # count syn tfs associated with syntenic blocks\n",
    "    ########\n",
    "    tfsyn_base = enh[[\"enh_id\",\"chr_syn\", \"start_syn\", \"end_syn\", \"syn_id\",\n",
    "              \"seg_index\", \"core_remodeling\", \"core\", \"mrca\", \"syn_len\"]].drop_duplicates()\n",
    "\n",
    "    tfsyn = enh.loc[enh[\"len_tfbs_overlap\"]>=6].groupby([\"enh_id\",\\\n",
    "                                                         \"syn_id\",\"seg_index\", \"syn_len\"])[\"tf\"].count().reset_index()\n",
    "    \n",
    "\n",
    "    tfsyn.columns=[\"enh_id\", \"syn_id\",\"seg_index\", \"syn_len\", 'syn_tf_count'] # rename columns\n",
    "\n",
    "    tfsyn[\"syn_tf_den\"] = tfsyn[\"syn_tf_count\"].divide(tfsyn[\"syn_len\"])\n",
    "\n",
    "    # count uniq syn tfs associated with syntenic blocks\n",
    "\n",
    "    tfsynunq = enh.loc[enh[\"len_tfbs_overlap\"]>=6].groupby([\"enh_id\",\"syn_id\",\"seg_index\",\"syn_len\"])[\"tf\"].unique().reset_index()\n",
    "\n",
    "\n",
    "    tfsynunq[\"syn_tf_unq_count\"] = tfsynunq[\"tf\"].apply(lambda x: len(x))# count the number of unique TFs\n",
    "    tfsynunq[\"syn_tf_unq_den\"] = tfsynunq[\"syn_tf_unq_count\"].divide(tfsynunq[\"syn_len\"])\n",
    "\n",
    "    syndfs = [tfsyn_base, tfsyn, tfsynunq]\n",
    "\n",
    "    syn_merged = reduce(lambda left,right: pd.merge(left,right,on=['enh_id','syn_id', 'seg_index', 'syn_len'],\n",
    "                                                how='left'), syndfs)\n",
    "\n",
    "    print(\"# total syn blocks\", len(tfsyn_base))\n",
    "    print(\"# total syn blocks overlapping tf >= 6bp\", len(tfsyn)) # exclude any syn w/o tfbs overlapping >=6bp\n",
    "    print(\"# total unq syn blocks overlapping tf >= 6bp\", len(tfsynunq)) # exclude any syn w/o uniq tfbs overlapping >=6bp\n",
    "    print(\"len merged syn\", len(syn_merged)) # the left-merged df\n",
    "\n",
    "    syn_merged.to_csv(\"%s%s_syn_tfbs_density.bed\" % (out_path, sample_id), sep= '\\t', header = True, index = False)\n",
    "\n",
    "    print(\"Finished calculating TFBS density\", datetime.now())  \n",
    "\n",
    "    \n",
    "def calculateExpected(test_enh, shuffle_id, test_path, species, iters):\n",
    "    BLACKLIST, CHROM_SZ = loadConstants(species)  # note CHROM_SZ not used\n",
    "    exp_sum = 0\n",
    "\n",
    "    shuffle_path = \"%s/shuffle\" % test_path # make a shuffle path file\n",
    "    os.system(\"mkdir %s\" % shuffle_path)\n",
    "\n",
    "    rand_file = BedTool(test_enh).shuffle(genome='hg19', excl=BLACKLIST, chrom=True, noOverlapping=True) # shuffle bed\n",
    "    rand_out = '%s/rand_file_%s.bed'% (shuffle_path, shuffle_id) # make shuffle file\n",
    "    rand_file.saveas(rand_out)# save shuffle\n",
    "\n",
    "    enh_age(rand_out, shuffle_id, shuffle_path, species) # age the shuffles\n",
    "\n",
    "    break_tfbs(shuffle_id, shuffle_path) # measure shuffle breaks\n",
    "    \n",
    "    #tfbs_density(shuffle_id, shuffle_path) # intersect shuffles with ENCODE TFBS\n",
    "###\n",
    "#   main\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_ENH = '/dors/capra_lab/projects/enhancer_ages/klein2018/data/merged.bed'\n",
    "SHUFFLE_ID = 'Shuf_test2-klein'\n",
    "TEST_PATH = '/dors/capra_lab/projects/enhancer_ages/klein2018/data'\n",
    "SPECIES = 'hg19'\n",
    "ITERATIONS = 3\n",
    "num_threads= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /accre/arch/easybuild/software/BinDist/Anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py -f /gpfs22/home/fongsl/.local/share/jupyter/runtime/kernel-faad2413-db6e-49b2-88ba-96adf17298a8.json 2019-08-02 14:40:20.\n",
      "concatenating aged enhancer chromosome files cat /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle/chr*_Shuf_test2-klein_ages.bed > /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle/Shuf_test2-klein_enh_ages.bed\n",
      "concatenating aged enhancer chromosome files cat /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle/chr*_Shuf_test2-klein_ages.bed > /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle/Shuf_test2-klein_enh_ages.bed\n",
      "Shuf_test2-klein /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle\n",
      "Shuf_test2-klein /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle\n",
      "unique enhancers = 797\n",
      "unique enhancers = 797\n",
      "# rows to reduce and assemble breaks =  12628\n",
      "# rows to reduce and assemble breaks =  12628\n",
      "Start assembling breaks 2019-08-02 14:41:43.850540\n",
      "Start assembling breaks 2019-08-02 14:41:43.856459\n",
      "Finished assembling breaks 2019-08-02 14:42:36.898910\n",
      "Finished assembling breaks 2019-08-02 14:42:36.900373\n",
      "concatenating aged enhancer chromosome files cat /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle/chr*_Shuf_test2-klein_ages.bed > /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle/Shuf_test2-klein_enh_ages.bed\n",
      "Shuf_test2-klein /dors/capra_lab/projects/enhancer_ages/klein2018/data/shuffle\n",
      "unique enhancers = 943\n",
      "# rows to reduce and assemble breaks =  14983\n",
      "Start assembling breaks 2019-08-02 14:44:01.360540\n",
      "Finished assembling breaks 2019-08-02 14:45:02.445956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main(argv):\n",
    "    print('python {:s} {:s}'.format(' '.join(sys.argv), str(datetime.now())[:20]))\n",
    "    \n",
    "#    enh_age(TEST_ENH, SAMPLE_ID, TEST_PATH, SPECIES) # age enhancers\n",
    " #   break_tfbs(SAMPLE_ID, TEST_PATH)\n",
    "  #  tfbs_density(SAMPLE_ID, TEST_PATH)\n",
    "    \n",
    "    # create pool and run simulations in parallel\n",
    "    pool = Pool(num_threads)\n",
    "    partial_calcExp = partial(calculateExpected, BedTool(TEST_ENH), SHUFFLE_ID, TEST_PATH, SPECIES)\n",
    "    exp_sum_list = pool.map(partial_calcExp, [i for i in range(ITERATIONS)])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv[1:])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
